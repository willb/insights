{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from keras.callbacks import History \n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import class_weight\n",
    "import sys\n",
    "import tensorflow\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .config(\"spark.executor.memory\", '32g') \\\n",
    "        .config(\"spark.driver.memory\", '32g') \\\n",
    "        .config(\"spark.local.dir\",\"/Volumes/Rob Backup/spark\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "np.set_printoptions(linewidth=150)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allParquets(data_path):\n",
    "    dfs = []\n",
    "    for file in os.listdir(data_path):\n",
    "        if file==\"mappings\":\n",
    "            dfs+=allParquets(data_path+\"mappings/\")\n",
    "            continue\n",
    "        if file==\"reports\" or file=='.DS_Store':\n",
    "            continue\n",
    "        df=spark.read.parquet(data_path+file).repartition(128)\n",
    "        dfs.append({'file':file,'data':df})\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group by Unique Incidents\n",
    "Account, system_id, and upload_time uniquely denote each individual incident in the data, so let's group all that data together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupIDs(dfs):\n",
    "    datasets = []\n",
    "    numDFs = len(dfs)\n",
    "    for i,df in enumerate(dfs):\n",
    "        filename = df['file']\n",
    "        print(\"{}/{}: {}\".format(i,numDFs,filename))\n",
    "        if 'mapping' in filename or 'yumlog_yumlog' in filename:\n",
    "            continue    \n",
    "        names   = [name for name in df['data'].schema.names if name not in ['account','system_id','upload_time']]\n",
    "        grouped = df['data'].groupBy('account','system_id','upload_time').agg(*[F.collect_list(name) for name in names])\n",
    "        mapped  = grouped.rdd.map(lambda x: ['{}_{}_{}'.format(x['account'],x['system_id'],x['upload_time']),\n",
    "                                             x.asDict()])\n",
    "        datasets.append({'file':filename,'data':mapped.toDF(['identity','data'])})\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinSelect(dfDict,num=\"\"):\n",
    "    return dfDict['data'].selectExpr(\"identity as identity{}\".format(num), \"data as {}\".format(dfDict['file']))\n",
    "\n",
    "def joinAll(datasets):\n",
    "    df = joinSelect(datasets[0])\n",
    "    for dataset in datasets[1:]:\n",
    "        df2 = joinSelect(dataset,2)\n",
    "        df = df.join(df2,df.identity==df2.identity2,\"outer\").drop(\"identity2\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== insights_parsers_uname_uname ===\n",
      "=== insights_parsers_hostname_hostname ===\n",
      "=== rule_data ===\n",
      "=== insights_parsers_ps_psauxcww ===\n",
      "=== insights_parsers_redhat_release_redhatrelease ===\n",
      "=== insights_parsers_meminfo_meminfo ===\n",
      "=== exec_times ===\n",
      "=== insights_parsers_docker_list_dockerlistimages ===\n",
      "=== insights_combiners_virt_what_virtwhat ===\n",
      "=== insights_parsers_installed_rpms_installedrpms ===\n",
      "=== insights_parsers_dmidecode_dmidecode ===\n",
      "=== insights_parsers_ps_psaux ===\n",
      "=== insights_parsers_cpuinfo_cpuinfo ===\n",
      "=== insights_parsers_yumlog_yumlog ===\n",
      "=== account_mapping ===\n",
      "=== system_id_account_mapping ===\n",
      "=== system_id_mapping ===\n",
      "=== insights_combiners_services_services ===\n",
      "=== errors ===\n",
      "=== insights_parsers_lsmod_lsmod ===\n",
      "=== insights_parsers_yum_yumrepolist ===\n"
     ]
    }
   ],
   "source": [
    "date = '2018-08-09'\n",
    "data_path=\"insights_data/{}/\".format(date)\n",
    "\n",
    "df = joinAll(groupIDs(allParquets(data_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.repartition(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"{}-{}\".format(date,'joined'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
